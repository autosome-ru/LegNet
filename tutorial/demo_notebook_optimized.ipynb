{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../dream_ablation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LegNet**: solving the sequence-to-expression problem with SOTA convolutional networks\n",
    "\n",
    "This is the tutorial demonstrating how **optimized** LegNet can be practically used with the data from yeast gigantic parallel reporter assays. \n",
    "\n",
    "The code below focuses on changes in the original LegNet pipeline. To follow the main ideas behidnd LegNet please refer to `demo_notebook.ipynb`. The main code and scripts to run training and testing for the optimized LegNet could be found in `dream_ablation` folder.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main optimization changes refer to the neural network architecture and optimizer choice. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/newlegnet.png\" width=\"500\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scheme above represents the optimized LegNet architecture. Blocks removed from the original architecture are highlighted in red. Red text labels denote modified parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SE block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized LegNet uses a standard EfficientNetV2 SE-block instead of an originally used custom variant. Commented lines refer to the custom variant. The full realization of custom variant can be found in `se_complex.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(oup, int(inp // reduction)),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(int(inp // reduction), oup),\n",
    "\n",
    "                # Concater(Bilinear(int(inp // reduction), int(inp // reduction // 2), rank=0.5, bias=True)),\n",
    "                # nn.SiLU(),\n",
    "                # nn.Linear(int(inp // reduction) +  int(inp // reduction // 2), oup),\n",
    "\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, = x.size()\n",
    "        y = x.view(b, c, -1).mean(dim=2)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetV2-like block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The optimized LegNet uses \n",
    "\n",
    " * depth-wise instead of grouped convolutions; \n",
    "\n",
    " * the original method of EfficientNetV2 to set the dimensionality of the EfficientNetV2-like block.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_ch, \n",
    "                 ks, \n",
    "                 resize_factor,\n",
    "                 filter_per_group,\n",
    "                 activation, \n",
    "                 out_ch=None,\n",
    "                 se_reduction=None,\n",
    "                 se_type=\"simple\",\n",
    "                 inner_dim_calculation=\"in\"\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = resize_factor if se_reduction is None else se_reduction\n",
    "        self.ks = ks\n",
    "        self.inner_dim_calculation = inner_dim_calculation\n",
    "\n",
    "        '''\n",
    "        `in` refers to the original method of EfficientNetV2 to set the dimensionality of the EfficientNetV2-like block\n",
    "        `out` is the mode used in the original LegNet approach\n",
    "\n",
    "        This parameter slighly changes the mechanism of channel number calculation \n",
    "        which can be seen in the figure above (C, channel number is highlighted in red).\n",
    "        '''\n",
    "        if inner_dim_calculation == \"out\":\n",
    "            self.inner_dim = self.out_ch * self.resize_factor\n",
    "        elif inner_dim_calculation == \"in\":\n",
    "            self.inner_dim = self.in_ch * self.resize_factor\n",
    "        else:\n",
    "            raise Exception(f\"Wrong inner_dim_calculation: {inner_dim_calculation}\")\n",
    "            \n",
    "        \n",
    "        self.filter_per_group = filter_per_group\n",
    "\n",
    "        se_constructor = SELayer\n",
    "\n",
    "        block = nn.Sequential(\n",
    "                        nn.Conv1d(\n",
    "                            in_channels=self.in_ch,\n",
    "                            out_channels=self.inner_dim,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.inner_dim),\n",
    "                       activation(),\n",
    "                       \n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.inner_dim,\n",
    "                            out_channels=self.inner_dim,\n",
    "                            kernel_size=ks,\n",
    "                            groups=self.inner_dim // self.filter_per_group,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.inner_dim),\n",
    "                       activation(),\n",
    "                       se_constructor(self.in_ch, \n",
    "                                      self.inner_dim,\n",
    "                                      reduction=self.se_reduction), # self.in_ch is not good\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.inner_dim,\n",
    "                            out_channels=self.in_ch,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.in_ch),\n",
    "                       activation(),\n",
    "        )\n",
    "        \n",
    "      \n",
    "        self.block = block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General architecture modifications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized LegNet does not include activation at the final layer before average pooling. \n",
    " \n",
    "However, we kept ResidualConcat as residual connections were important for reaching optimal scores (see Supplementary Table S1). We also kept the original total number of blocks which provide a 79 base pairs receptive field that is close to the actual variable length of the tested promoter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The `activation()` in the optimized architecture simply equals `nn.Identity`\n",
    "In the original LegNet approach it was `nn.SiLU`\n",
    "'''\n",
    "\n",
    "class MappingBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        self.block =  nn.Sequential(\n",
    "                        nn.Conv1d(\n",
    "                            in_channels=in_ch,\n",
    "                            out_channels=out_ch,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                       ),\n",
    "                       activation()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Residual concatenation block is implemented below and is common between optimized and original approaches\n",
    "'''\n",
    "\n",
    "import torch\n",
    "\n",
    "class ResidualConcat(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return torch.concat([self.fn(x, **kwargs), x], dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole optimized LegNet architecture is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "from typing import Type\n",
    "from local_block import LocalBlock\n",
    "    \n",
    "class LegNet(nn.Module):\n",
    "    \"\"\"\n",
    "    LegNet neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    use_single_channel : bool\n",
    "        If True, singleton channel is used.\n",
    "    block_sizes : list, optional\n",
    "        List containing block sizes. The default is [256, 256, 128, 128, 64, 64, 32, 32].\n",
    "    ks : int, optional\n",
    "        Kernel size of convolutional layers. The default is 5.\n",
    "    resize_factor : int, optional\n",
    "        Resize factor used in a high-dimensional middle layer of an EffNet-like block. The default is 4.\n",
    "    activation : nn.Module, optional\n",
    "        Activation function. The default is nn.SiLU.\n",
    "    filter_per_group : int, optional\n",
    "        Number of filters per group in a middle convolutiona layer of an EffNet-like block. The default is 2.\n",
    "    se_reduction : int, optional\n",
    "        Reduction number used in SELayer. The default is 4.\n",
    "    final_ch : int, optional\n",
    "        Number of channels in the final output convolutional channel. The default is 18.\n",
    "    bn_momentum : float, optional\n",
    "        BatchNorm momentum. The default is 0.1.\n",
    "\n",
    "    \"\"\"\n",
    "    __constants__ = ('resize_factor')\n",
    "    \n",
    "    def __init__(self, \n",
    "                use_single_channel: bool, \n",
    "                use_reverse_channel: bool,\n",
    "                block_sizes: list[int]=[256, 128, 128, 64, 64, 64, 64], \n",
    "                ks: int=7, \n",
    "                resize_factor: int=4, \n",
    "                activation: Type[nn.Module]=nn.SiLU,\n",
    "                final_activation: Type[nn.Module]=nn.Identity,\n",
    "                filter_per_group: int=1,\n",
    "                se_reduction: int=4,\n",
    "                res_block_type: str=\"concat\",\n",
    "                se_type: str=\"simple\",\n",
    "                inner_dim_calculation: str=\"in\"):        \n",
    "        super().__init__()\n",
    "        self.block_sizes = block_sizes\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = se_reduction\n",
    "        self.use_single_channel = use_single_channel\n",
    "        self.use_reverse_channel = use_reverse_channel\n",
    "        self.filter_per_group = filter_per_group\n",
    "        self.final_ch = 18 # number of bins in the competition\n",
    "        self.inner_dim_calculation= inner_dim_calculation\n",
    "        self.res_block_type = res_block_type\n",
    "        \n",
    "\n",
    "        residual = ResidualConcat\n",
    "        \n",
    "        self.stem_block = LocalBlock(in_ch=self.in_channels,\n",
    "                           out_ch=block_sizes[0],\n",
    "                           ks=ks,\n",
    "                           activation=activation)\n",
    "\n",
    "        blocks = []\n",
    "        for ind, (prev_sz, sz) in enumerate(zip(block_sizes[:-1], block_sizes[1:])):\n",
    "            block = nn.Sequential(\n",
    "                residual(EffBlock(in_ch=prev_sz, \n",
    "                         out_ch=sz,\n",
    "                         ks=ks,\n",
    "                         resize_factor=4,\n",
    "                         activation=activation,\n",
    "                         filter_per_group=self.filter_per_group,\n",
    "                         se_type=se_type,\n",
    "                         inner_dim_calculation=inner_dim_calculation)),\n",
    "                LocalBlock(in_ch=2 * prev_sz,\n",
    "                               out_ch=sz,\n",
    "                               ks=ks,\n",
    "                               activation=activation)\n",
    "            )\n",
    "            blocks.append(block)\n",
    "\n",
    "        \n",
    "        self.main = nn.Sequential(*blocks)\n",
    "\n",
    "        self.mapper =  MappingBlock(in_ch=block_sizes[-1],\n",
    "                                    out_ch=self.final_ch,\n",
    "                                    activation=final_activation)\n",
    "        \n",
    "        \n",
    "        self.register_buffer('bins', torch.arange(start=0, end=18, step=1, requires_grad=False))\n",
    "\n",
    "    @property\n",
    "    def in_channels(self) -> int:\n",
    "        return 4 + self.use_reverse_channel + self.use_single_channel\n",
    "    \n",
    "    def forward(self, x):    \n",
    "        x = self.stem_block(x)\n",
    "        x = self.main(x)\n",
    "        x = self.mapper(x)\n",
    "        x = F.adaptive_avg_pool1d(x, 1)\n",
    "        x = x.squeeze(2)\n",
    "        logprobs = F.log_softmax(x, dim=1) \n",
    "        x = F.softmax(x, dim=1)\n",
    "        score = (x * self.bins).sum(dim=1)\n",
    "        return logprobs, score\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
